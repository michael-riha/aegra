import os
from typing import Any, TypedDict
from langchain_core.messages import (
    AIMessage,
    ToolMessage,
    convert_to_openai_messages,
)

from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, StateGraph
from langgraph.prebuilt import ToolNode
from langgraph.runtime import Runtime

# from utils.listener import MyCustomHandler
from mcp_agent.utils.logger import setup_logger
from mcp_agent.utils.helper import load_chat_model
from langgraph.runtime import Runtime

from mcp_agent.context import Context
from mcp_agent.state import AgentState

# Set up logger for this module
logger = setup_logger(__name__)

MCP_URL = os.environ.get("MCP_URL", "http://host.docker.internal:8001/mcp")

# Setup function that can be awaited as the MCP Client is async
async def setup_tools_async():
    from langchain_mcp_adapters.client import MultiServerMCPClient

    client = MultiServerMCPClient(
        {
            "math": {
                "url": MCP_URL,
                "transport": "streamable_http",
            }
        }
    )
    # Initialize the MCP tool
    tools = await client.get_tools()
    return tools


# Async node function that uses the pre-initialized components
async def agent_node(state: AgentState, runtime: Runtime[Context]) -> AgentState:
    llm, tools = await get_components(runtime)
    runtime.context.tools = tools
    runtime.context.llm= llm
    # Create the tool node
    tool_node = ToolNode(tools)
    runtime.context.toolNode = tool_node
    # The convert_to_openai_messages utility function can be used to convert from LangChain messages to OpenAI format.
    # oai_messages = {"messages": convert_to_openai_messages(state['messages'][0]['content'][0]['text'])}
    # Use the pre-initialized agent
    # logger.info("Agent input: %s", oai_messages)  # Changed from print
    messages = process_messages_request(state)
    response = await llm.ainvoke(messages)
    logger.debug(
        "Response type: %s", response.type if hasattr(response, "type") else "unknown"
    )  # Changed from print
    logger.debug(
        "Tool calls: %s",
        response.tool_calls if hasattr(response, "tool_calls") else "none",
    )  # Changed from print
    logger.info(
        "Response content: %s",
        response.content if hasattr(response, "content") else str(response),
    )  # Changed from print

    # Check if the response has tool calls
    if not hasattr(response, "tool_calls") or not response.tool_calls:
        logger.warning("No tool calls were generated by the LLM")  # Changed from print
    state["messages"].append(response)  # Keep history
    return {"messages": state["messages"]}  # Return updated history

def process_messages_request(state):
    # For AWS Bedrock or similar services, we need to format content according to their requirements
    messages = []
    
    # Process first message content items
    content_items = []
    
    for item in state['messages'][0]['content']:
        if item['type'] == 'text':
            # Add text content
            content_items.append({
                "type": "text",
                "text": item['text']
            })
        elif item['type'] == 'file' and item['mime_type'] == 'application/pdf':
            # Add document content for PDF
            content_items.append({
                "type": "document",
                "source": {
                    "type": "base64",
                    "media_type": "application/pdf",
                    "data": item['data']
                }
            })
        elif item['type'] == 'image' or (item['type'] == 'file' and item['mime_type'].startswith('image/')):
            # Add image content
            content_items.append({
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": item.get('mime_type', 'image/jpeg'),  # Default to jpeg if not specified
                    "data": item['data'] if 'data' in item else item.get('source_type', {}).get('data', '')
                },
                "metadata": item.get('metadata', {})
            })
    
    # Create the message with all content items
    messages.append({
        "role": "user",
        "content": content_items
    })
    return messages

# Tool node function
async def tool_node(state: AgentState, runtime: Runtime[Context]) -> AgentState:
    # Get the last message (which should have tool calls)
    last_message = state["messages"][-1]
    tool_node = runtime.context.toolNode

    # Create a proper input for the tool node
    if not hasattr(last_message, "tool_calls") and isinstance(last_message, dict):
        # If it's a dict with tool_calls field
        if "tool_calls" in last_message:
            # Convert the dict to a proper AIMessage
            tool_message = AIMessage(content="", tool_calls=last_message["tool_calls"])
            tool_input = {"messages": [tool_message]}
        else:
            raise ValueError(f"Last message doesn't have tool_calls: {last_message}")
    else:
        # If it's already a message object with tool_calls
        tool_input = {"messages": [last_message]}

    # Debug output to see what we're sending
    logger.debug("Tool input: %s", tool_input)  # Changed from print

    # Execute the tool
    tool_result = await tool_node.ainvoke(tool_input)

    # Add the tool result to messages
    if "messages" in tool_result and tool_result["messages"]:
        state["messages"].append(tool_result["messages"][-1])
    else:
        logger.warning(
            "Unexpected tool result format: %s", tool_result
        )  # Changed from print
        # Create a default tool message if needed
        state["messages"].append(
            ToolMessage(
                content="Tool execution completed but returned unexpected format"
            )
        )

    return {"messages": state["messages"]}


# Route function to decide what to do next
def should_call_tool(state: AgentState) -> str:
    """
    Determines if we should continue with the agent or use tools
    """
    logger.debug("should_continue called with state: %s", state)  # Changed from print

    messages = state.get("messages")
    if not messages:
        logger.debug("No messages, returning 'agent'")  # Changed from print
        return "agent"

    last_message = messages[-1]
    logger.debug("Last message: %s", last_message)  # Changed from print

    # Check if the agent wants to use tools
    has_tool_calls = hasattr(last_message, "tool_calls") and last_message.tool_calls
    logger.debug("Has tool calls: %s", has_tool_calls)  # Changed from print

    if has_tool_calls:
        logger.debug("Routing to 'tools'")  # Changed from print
        return "tools"
    else:
        logger.debug("Routing to 'end'")  # Changed from print
        return "end"

async def get_components(runtime: Runtime[Context]):
    # Initialize ChatOpenAI to use AgentGateway
    # Initialize the model with tool binding. Change the model or add more tools here.
    llm = load_chat_model(runtime.context.model)

    tools = await setup_tools_async()
    llm_with_tools = llm.bind_tools(tools)
    return llm_with_tools, tools


# Build LangGraph workflow
builder = StateGraph(AgentState, context_schema=Context)
builder.add_node("agent", agent_node)
builder.add_node("tools", tool_node)
builder.add_edge(START, "agent")
builder.add_conditional_edges(
    "agent",
    should_call_tool,
    {
        "tools": "tools",
        "end": END,
        "agent": "agent",  # Allow looping back to agent if needed
    },
)
builder.add_conditional_edges(
    "tools",
    should_call_tool,
    {"end": END, "agent": "agent"},  # Allow looping back to agent if needed
)
graph = builder.compile()
